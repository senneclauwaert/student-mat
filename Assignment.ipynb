{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student Name:** [Your Name Here]  \n",
    "**Assignment:** Machine Learning Model Comparison for Student Grade Prediction  \n",
    "**Date:** August 2025  \n",
    "**Course:** [Your Course Name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Predicting Student Math Grades Using Machine Learning\n\n## What We Will Do\n\nThis project predicts how well students will do in math class. We use information about students to guess their final grade.\n\nWe will test 3 different computer programs (algorithms):\n1. **Decision Tree** - Makes choices like a flowchart\n2. **Random Forest** - Uses many decision trees together  \n3. **K-Nearest Neighbors** - Looks at similar students\n\n## Why This Is Important\n\nTeachers can find students who need help early. This helps students pass their classes.\n\n## What You Will Learn\n\n- How to clean data for machine learning\n- How to train 3 different models\n- How to pick the best model\n- How to make an API (web service) for predictions"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The deadline for the notebook is 25/08/2025</b>.\n",
    "\n",
    "\n",
    "<b>The deadline for the video is 29/08/2025</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## About Our Data\n\n### The Student Dataset\n\nWe have data about 395 students from 2 schools in Portugal. \n\n**What we know about each student:**\n- **Personal info**: Age, gender, where they live\n- **Family info**: Parents' jobs and education\n- **School info**: Study time, past grades, absences\n- **Social info**: Free time, relationships, going out\n\n**Our goal**: Predict the final math grade (0 to 20 points)\n\n### Why This Data Is Good\n\nThis data has many different types of information about students. This helps us make better predictions than just using grades alone."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Loading The Data (Simple Version)\n\nThis code loads our data and splits it into training and testing parts.\n\n**What happens here:**\n1. **Load data** from the CSV file\n2. **Mix up the data** so it's random\n3. **Split data**: 80% for training, 20% for testing\n4. **Get features and target**: Separate student info from grades\n\n**Note**: This is the basic way to load data. We will do better data preparation later."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as pl\n",
    "%matplotlib inline\n",
    "\n",
    "#read and randomly shuffle data\n",
    "mathscores = pd.read_csv('student-mat.csv', sep=';')\n",
    "\n",
    "features = mathscores.columns[1:]\n",
    "\n",
    "mathscores = mathscores.values\n",
    "mathscores = mathscores[np.random.permutation(mathscores.shape[0]),:]\n",
    "\n",
    "#80% - 20% split for the training and testing sets\n",
    "tr_set_size = int(len(mathscores)*0.8)  # Fixed: changed np.int to int\n",
    "\n",
    "#assign train and test sets (in your experiments, you want to do cross-validation)\n",
    "X_tr = mathscores[0:tr_set_size,:30]\n",
    "y_tr = mathscores[0:tr_set_size,32]\n",
    "X_test = mathscores[tr_set_size:,:30]\n",
    "y_test = mathscores[tr_set_size:,32]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimum Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to train at least 3 different models on the data set. Make sure to include the reason for your choice (e.g., for dealing with categorical features).\n",
    "\n",
    "* Define the problem, analyze the data, prepare the data for your model.\n",
    "* Train at least 3 models (e.g. decision trees, nearest neighbour, ...) to predict whether a mushroom is of poisonous or edible. You are allowed to use any machine learning model from scikit-learn or other methods, as long as you motivate your choice.\n",
    "* For each model, optimize the model parameters settings (tree depth, hidden nodes/decay, number of neighbours,...). Show which parameter setting gives the best model.\n",
    "* Compare the best parameter settings for the models and estimate their errors on unseen data. Investigate the learning process critically (overfitting/underfitting). Can you show that one of the models performs better?\n",
    "\n",
    "All results, plots and code should be handed in as an interactive <a href='http://ipython.org/notebook.html'>iPython notebook</a>. Simply providing code and plots does not suffice, you are expected to accompany each technical section by explanations and discussions on your choices/results/observation/etc in the notebook and in a video (by recording your screen en voice). \n",
    "\n",
    "<b>The deadline for the notebook is 25/08/2025</b>.\n",
    "\n",
    "<b>The deadline for the video is 29/08/2025</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are encouraged to try and see if you can further improve on the models you obtained above. This is not necessary to obtain a good grade on the assignment, but any extensions on the minimum requirements will count for extra credit. Some suggested possibilities to extend your approach are:\n",
    "\n",
    "* Build and host an API for your best performing model. You can create a API using pyhton frameworks such as FastAPI, Flask, ... You can host een API for free on Render, using your student credit on Azure, ...\n",
    "* Try to combine multiple models. Ensemble and boosting methods try to combine the predictions of many, simple models. This typically works best with models that make different errors. Scikit-learn has some support for this, <a href=\"http://scikit-learn.org/stable/modules/ensemble.html\">see here</a>. You can also try to combine the predictions of multiple models manually, i.e. train multiple models and average their predictions\n",
    "* You can always investigate whether all features are necessary to produce a good model. Feel free to lookup additional resources and papers to find more information, see e.g <a href='https://scikit-learn.org/stable/modules/feature_selection.html'> here </a> for the feature selection module provided by scikit-learn library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Depending on the model used, you may want to <a href='http://scikit-learn.org/stable/modules/preprocessing.html'>scale</a> or <a href='https://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features'>encode</a> your (categorical) features X and/or outputs y\n",
    "* Refer to the <a href='http://scipy.org/docs.html'>SciPy</a> and <a href='http://scikit-learn.org/stable/documentation.html'>Scikit learn</a> documentations for more information on classifiers and data handling.\n",
    "* You are allowed to use additional libraries, but provide references for these.\n",
    "* The assignment is **individual**. All results should be your own. Plagiarism will not be tolerated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mathscores_csv = pd.read_csv('student-mat.csv', sep=';')\n",
    "\n",
    "mathscores_csv = mathscores_csv[features].dropna()\n",
    "\n",
    "mathscores_csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Preparing Data for Machine Learning\n\n## The Problem We Want to Solve\n\n**Simple goal**: Look at student information and guess their final math grade.\n\n**Why this helps**: Teachers can find students who might fail and help them early.\n\n## How We Measure Success\n\nWe use 3 ways to check if our predictions are good:\n- **MSE**: How far off our guesses are (lower = better)\n- **MAE**: Average error in grade points (lower = better)  \n- **RÂ²**: How much of the grade pattern we can explain (higher = better)\n\n## Our Step-by-Step Plan\n\n1. **Look at the data** - Understand what we have\n2. **Clean the data** - Fix problems and prepare it\n3. **Train 3 models** - Teach computers to predict grades\n4. **Test models** - See which one works best\n5. **Save the best model** - Use it to help real students"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "### Step 1: Look at Our Data\n\n**What we import:**\n- **pandas**: For working with data tables\n- **numpy**: For math calculations\n- **matplotlib/seaborn**: For making charts\n- **scikit-learn**: For machine learning\n\n**What this code does:**\n1. **Load all our data** into the computer\n2. **Check the data** - How big is it? Any missing parts?\n3. **Look at grades** - What do final grades look like?\n4. **Make charts** - Pictures help us understand data better\n\nThis step helps us understand our data before we start training models."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Step 2: Clean and Prepare Data\n\n**Why we need to do this:**\nComputers only understand numbers, but our data has words like \"male\", \"female\", \"urban\", \"rural\".\n\n**What this code does:**\n1. **Find text columns** - Things like gender, school name\n2. **Turn words into numbers** - \"male\"=0, \"female\"=1\n3. **Split our data** - 80% for training, 20% for testing\n4. **Scale numbers** - Make all numbers similar size (needed for some models)\n\n**Important**: We keep the same random split every time so we can compare results fairly."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Model 1: Decision Tree\n\n## What is a Decision Tree?\n\nThink of a decision tree like asking questions to guess a student's grade:\n- \"Did they fail classes before?\" â If yes, probably lower grade\n- \"Do they study a lot?\" â If yes, probably higher grade\n- \"What were their past grades?\" â Higher past grades = higher final grade\n\n## Why Use Decision Tree?\n\n**Good things:**\n- **Easy to understand** - We can see exactly how it makes decisions\n- **Works with any data** - Numbers and text both work\n- **No special preparation** - Don't need to scale numbers\n- **Shows what's important** - Tells us which student factors matter most\n\n**Problems:**\n- **Can memorize training data** - Might not work on new students\n- **Not always stable** - Small data changes can make big differences\n\n## Settings We Will Test\n\n- **max_depth**: How many questions deep the tree goes\n- **min_samples_split**: How many students needed to ask a new question\n- **min_samples_leaf**: Minimum students in each final answer"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "### Training the Decision Tree\n\n**What this code does:**\n\n1. **Makes a function to test models** - We use this for all 3 models\n2. **Tests many different settings** - Tries 45 different combinations\n3. **Uses cross-validation** - Tests each setting 5 times to be sure\n4. **Picks the best settings** - Chooses what works best\n5. **Tests on new data** - Sees how well it works on students it never saw\n\n**How we find the best settings:**\n- Try different tree depths (3, 5, 7, 10, or unlimited)\n- Try different minimum students per question (2, 5, or 10)\n- Try different minimum students per answer (1, 2, or 4)\n\n**What the results mean:**\n- **Training scores** - How well it works on students it learned from\n- **Test scores** - How well it works on new students (this is what matters!)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Model 2: Random Forest\n\n## What is Random Forest?\n\nRandom Forest is like asking many different teachers to guess a student's grade, then taking the average of all their guesses.\n\nEach \"teacher\" (tree) looks at:\n- Different students from the training data\n- Different questions about each student\n\nThen we average all the guesses to get the final answer.\n\n## Why Use Random Forest?\n\n**Good things:**\n- **More accurate** - Usually better than one decision tree\n- **Less memorizing** - Harder to memorize training data\n- **Shows what's important** - Tells us which student factors matter most\n- **Works with any data** - Numbers and text both work\n- **More stable** - Small data changes don't matter as much\n\n**Problems:**\n- **Harder to understand** - Can't easily see how it makes decisions\n- **Slower to train** - Has to make many trees instead of one\n\n## Settings We Will Test\n\n- **n_estimators**: How many trees to make (50, 100, or 200)\n- **max_depth**: How deep each tree goes\n- **min_samples_split**: Minimum students to make a new question\n- **min_samples_leaf**: Minimum students in each final answer"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Training Random Forest and Finding Important Features\n\n**What this code does:**\n\n1. **Tests many settings** - Tries 108 different combinations\n2. **Trains the best model** - Uses the settings that work best\n3. **Finds important features** - Shows which student info matters most\n4. **Makes a chart** - Shows the top 10 most important things\n\n**Feature importance explained:**\n- Numbers add up to 1.0 (100%)\n- Higher numbers mean more important\n- Shows which student information helps predict grades the most\n\n**What we expect to be important:**\n- **Past grades (G1, G2)** - Previous grades predict final grades\n- **Number of failures** - Students who failed before might struggle\n- **Study time** - Students who study more usually do better\n- **Parent education** - Family background affects student success"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Model 3: K-Nearest Neighbors (KNN)\n\n## What is K-Nearest Neighbors?\n\nKNN is like asking: \"Show me students who are similar to this new student. What grades did they get?\"\n\nFor example, to predict Ana's grade:\n- Find 5 students most similar to Ana\n- Look at their final grades: 12, 14, 15, 13, 16\n- Average = 14, so we predict Ana will get 14\n\n## Why Use KNN?\n\n**Good things:**\n- **Simple idea** - Easy to understand concept\n- **No training needed** - Just stores all student data\n- **Finds patterns** - Can find complex relationships\n- **Works for similar cases** - Good when students are very similar\n\n**Problems:**\n- **Slow predictions** - Must check all students every time\n- **Needs scaled data** - Age (15-22) vs Absences (0-93) need same scale\n- **Sensitive to noise** - One weird student can mess up predictions\n- **Uses lots of memory** - Must store all training data\n\n## Settings We Will Test\n\n- **n_neighbors**: How many similar students to look at (3, 5, 7, 9, 11, 15)\n- **weights**: Should closer students count more? (uniform vs distance)\n- **metric**: How do we measure similarity? (euclidean vs manhattan)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "### Training KNN (Important: Uses Scaled Data)\n\n**Why scaling is important:**\nWithout scaling, age (15-22) and absences (0-93) have different ranges. KNN will think absences are more important just because the numbers are bigger!\n\n**What this code does:**\n\n1. **Uses scaled data** - All numbers are made similar size\n2. **Tests 24 combinations** - Different numbers of neighbors and settings\n3. **Finds best settings** - Which combination works best\n4. **Tests on new students** - How well does it predict?\n\n**What we expect:**\n- **Training error = 0** - KNN can perfectly remember all training students\n- **Test error > 0** - The real test is on new students\n- **Best k around 5-11** - Not too few (overfitting) or too many (underfitting)\n\n**Parameter meanings:**\n- **n_neighbors**: How many similar students to look at\n- **weights**: 'uniform' = all neighbors equal, 'distance' = closer neighbors matter more\n- **metric**: 'euclidean' = straight line distance, 'manhattan' = city block distance"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Comparing All Models and Picking the Best One\n\n## What This Section Does\n\nNow we compare all 3 models to see which one works best on new students.\n\n### How We Compare Models\n\n**We look at:**\n- **Test MSE**: How far off are our predictions? (lower = better)\n- **Test RÂ²**: How much of the grade pattern do we explain? (higher = better)\n- **Overfitting**: Does the model memorize training data too much?\n\n### Overfitting Check\n\n**What is overfitting?**\nWhen a model memorizes training data instead of learning patterns. It works great on training data but badly on new data.\n\n**How we check:**\n- Big difference between training and test scores = overfitting\n- Small difference = good model that generalizes well\n\n**What this code does:**\n\n1. **Makes a table** - Shows all model results together\n2. **Finds the best model** - Lowest test error wins\n3. **Checks for overfitting** - Compares training vs test performance\n4. **Makes charts** - Visual comparison of all models\n5. **Saves the winner** - Best model is saved for the API\n6. **Creates files** - Everything needed to use the model later"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Final Results and What We Learned\n\n## What We Did\n\nWe trained 3 different computer programs to predict student math grades:\n1. **Decision Tree** - Makes decisions like a flowchart\n2. **Random Forest** - Uses many decision trees together\n3. **K-Nearest Neighbors** - Looks at similar students\n\n## What We Found\n\n### Which Student Factors Matter Most\n\nFrom our analysis, these things help predict student success:\n1. **Past grades** - Students with good G1 and G2 grades usually get good final grades\n2. **Previous failures** - Students who failed before are more likely to struggle\n3. **Study time** - Students who study more hours do better\n4. **Family education** - Parents with more education often have kids who do better\n5. **Age and school factors** - These also matter but less\n\n### Best Model\n\nThe model with the lowest test error is our winner. This model will help teachers find students who need extra help.\n\n## How This Helps Teachers\n\n**Early Warning System**: Teachers can use this to:\n- Find students who might fail before it's too late\n- Give extra help to students who need it most\n- Use their time and resources better\n\n**What Makes Students Successful**: \n- Good study habits are very important\n- Family support matters\n- Past performance predicts future performance\n\n## What We Did Well\n\n- â Tested multiple algorithms fairly\n- â Used proper data splitting (no cheating!)\n- â Found the best settings for each model\n- â Checked for overfitting\n- â Made it ready for real-world use (API)\n- â Found which student factors matter most\n\n## Conclusion\n\nThis project shows how machine learning can help education. By looking at student information, we can predict who needs help and provide support early. This could help more students succeed in school.\n\nThe best model is now saved and ready to use through a web API that teachers or schools can use to help their students."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model Comparison and Deployment\n",
    "\n",
    "## Comprehensive Model Evaluation\n",
    "\n",
    "This section provides the final comparison of all three models and selects the best performer for deployment.\n",
    "\n",
    "### Evaluation Strategy\n",
    "\n",
    "**Multiple Metrics Approach:**\n",
    "- **MSE (Mean Squared Error)**: Penalizes large errors more heavily\n",
    "- **MAE (Mean Absolute Error)**: Average absolute prediction error\n",
    "- **RÂ² Score**: Proportion of variance explained by the model\n",
    "\n",
    "**Overfitting Analysis:**\n",
    "We analyze the gap between training and test performance to identify overfitting:\n",
    "- **Large gap**: Model memorizes training data (overfitting)\n",
    "- **Small gap**: Good generalization ability\n",
    "- **Negative gap**: Possible underfitting (rare)\n",
    "\n",
    "### What this code does:\n",
    "\n",
    "1. **Creates comparison table**: Organizes all metrics for easy comparison\n",
    "2. **Identifies best model**: Selects model with lowest test MSE\n",
    "3. **Analyzes overfitting**: Calculates training-test performance gaps\n",
    "4. **Generates visualizations**: Creates bar plots for visual comparison\n",
    "5. **Saves best model**: Exports model and preprocessing objects for production use\n",
    "6. **Deployment preparation**: Creates all files needed for API deployment\n",
    "\n",
    "### Expected Outcomes:\n",
    "\n",
    "**Typical Performance Ranking:**\n",
    "1. **Random Forest**: Usually best balance of accuracy and generalization\n",
    "2. **Decision Tree**: Good interpretability but may overfit\n",
    "3. **KNN**: Can work well but sensitive to hyperparameters and scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Random Forest\n",
    "\n",
    "## Why Random Forest?\n",
    "I chose Random Forest because:\n",
    "1. It's an ensemble method that combines multiple decision trees, reducing overfitting\n",
    "2. It handles both categorical and numerical features well\n",
    "3. It provides feature importance rankings\n",
    "4. It's generally more robust and accurate than single decision trees\n",
    "5. It can handle missing values and doesn't require feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training and Testing Random Forest\n",
    "\n",
    "print(\"Optimizing Random Forest hyperparameters...\")\n",
    "\n",
    "rf_params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "rf_grid = GridSearchCV(RandomForestRegressor(), rf_params, cv=5, \n",
    "                       scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "rf_grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {rf_grid.best_params_}\")\n",
    "print(f\"Best CV score (MSE): {-rf_grid.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate the best Random Forest model\n",
    "rf_results = evaluate_model(rf_grid.best_estimator_, X_train, X_test, y_train, y_test, \"Random Forest\")\n",
    "\n",
    "# Feature importance analysis\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': features,\n",
    "    'importance': rf_grid.best_estimator_.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\nTop 10 Most Important Features:\")\n",
    "for i, row in feature_importance.head(10).iterrows():\n",
    "    print(f\"{row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=feature_importance.head(10), x='importance', y='feature')\n",
    "plt.title('Top 10 Feature Importance - Random Forest')\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: K-Nearest Neighbors\n",
    "\n",
    "## Why K-Nearest Neighbors?\n",
    "I chose KNN because:\n",
    "1. It's a simple, non-parametric algorithm that can capture complex patterns\n",
    "2. It works well when there are clear clusters of similar students with similar grades\n",
    "3. It's intuitive - it predicts based on the grades of the most similar students\n",
    "4. It can handle both linear and non-linear relationships\n",
    "5. However, it requires feature scaling since it's distance-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training and Testing K-Nearest Neighbors\n",
    "\n",
    "print(\"Optimizing KNN hyperparameters...\")\n",
    "\n",
    "knn_params = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11, 15],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "# Use scaled features for KNN (important for distance-based algorithms)\n",
    "knn_grid = GridSearchCV(KNeighborsRegressor(), knn_params, cv=5, \n",
    "                        scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "knn_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Best parameters: {knn_grid.best_params_}\")\n",
    "print(f\"Best CV score (MSE): {-knn_grid.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate KNN with scaled features (since it's distance-based)\n",
    "def evaluate_knn_model(model, X_train_scaled, X_test_scaled, y_train, y_test, model_name):\n",
    "    \"\"\"Function to evaluate KNN model with scaled features\"\"\"\n",
    "    y_pred_train = model.predict(X_train_scaled)\n",
    "    y_pred_test = model.predict(X_test_scaled)\n",
    "    \n",
    "    train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "    test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "    train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "    test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "    train_r2 = r2_score(y_train, y_pred_train)\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "    \n",
    "    print(f\"\\n{model_name} Results:\")\n",
    "    print(f\"Training MSE: {train_mse:.4f}, Test MSE: {test_mse:.4f}\")\n",
    "    print(f\"Training MAE: {train_mae:.4f}, Test MAE: {test_mae:.4f}\")\n",
    "    print(f\"Training RÂ²: {train_r2:.4f}, Test RÂ²: {test_r2:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model, 'train_mse': train_mse, 'test_mse': test_mse,\n",
    "        'train_mae': train_mae, 'test_mae': test_mae,\n",
    "        'train_r2': train_r2, 'test_r2': test_r2\n",
    "    }\n",
    "\n",
    "# Evaluate the best KNN model\n",
    "knn_results = evaluate_knn_model(knn_grid.best_estimator_, X_train_scaled, X_test_scaled, \n",
    "                                 y_train, y_test, \"K-Nearest Neighbors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conclusion - Model Comparison and Saving\n",
    "\n",
    "# Create comparison table\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': ['Decision Tree', 'Random Forest', 'K-Nearest Neighbors'],\n",
    "    'Test MSE': [dt_results['test_mse'], rf_results['test_mse'], knn_results['test_mse']],\n",
    "    'Test MAE': [dt_results['test_mae'], rf_results['test_mae'], knn_results['test_mae']],\n",
    "    'Test RÂ²': [dt_results['test_r2'], rf_results['test_r2'], knn_results['test_r2']],\n",
    "    'Train MSE': [dt_results['train_mse'], rf_results['train_mse'], knn_results['train_mse']],\n",
    "    'Train RÂ²': [dt_results['train_r2'], rf_results['train_r2'], knn_results['train_r2']]\n",
    "})\n",
    "\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "print(results_df.round(4))\n",
    "\n",
    "# Find best model\n",
    "best_model_idx = results_df['Test MSE'].idxmin()\n",
    "best_model_name = results_df.loc[best_model_idx, 'Model']\n",
    "print(f\"\\nð Best performing model: {best_model_name}\")\n",
    "\n",
    "# Overfitting analysis\n",
    "print(f\"\\nð Overfitting Analysis (Train RÂ² - Test RÂ²):\")\n",
    "for i, model_name in enumerate(results_df['Model']):\n",
    "    overfitting = results_df.loc[i, 'Train RÂ²'] - results_df.loc[i, 'Test RÂ²']\n",
    "    status = \"High overfitting\" if overfitting > 0.1 else \"Good generalization\"\n",
    "    print(f\"{model_name}: {overfitting:.4f} ({status})\")\n",
    "\n",
    "# Visualize comparison\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.barplot(data=results_df, x='Model', y='Test MSE')\n",
    "plt.title('Test MSE (Lower is Better)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.barplot(data=results_df, x='Model', y='Test RÂ²')\n",
    "plt.title('Test RÂ² (Higher is Better)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "overfitting_data = results_df['Train RÂ²'] - results_df['Test RÂ²']\n",
    "sns.barplot(x=results_df['Model'], y=overfitting_data)\n",
    "plt.title('Overfitting Score (Lower is Better)')\n",
    "plt.ylabel('Train RÂ² - Test RÂ²')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the best model\n",
    "import joblib\n",
    "\n",
    "if best_model_name == 'Random Forest':\n",
    "    best_model = rf_grid.best_estimator_\n",
    "elif best_model_name == 'Decision Tree':\n",
    "    best_model = dt_grid.best_estimator_\n",
    "else:\n",
    "    best_model = knn_grid.best_estimator_\n",
    "\n",
    "# Save all necessary files\n",
    "joblib.dump(best_model, 'best_model.pkl')\n",
    "joblib.dump(label_encoders, 'label_encoders.pkl')\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "joblib.dump(features, 'features.pkl')\n",
    "\n",
    "print(f\"\\nð¾ Model files saved successfully!\")\n",
    "print(f\"â best_model.pkl - {best_model_name} model\")\n",
    "print(f\"â label_encoders.pkl - Categorical encoders\")\n",
    "print(f\"â scaler.pkl - Feature scaler\")\n",
    "print(f\"â features.pkl - Feature names\")\n",
    "print(f\"\\nReady to run API! ð\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}